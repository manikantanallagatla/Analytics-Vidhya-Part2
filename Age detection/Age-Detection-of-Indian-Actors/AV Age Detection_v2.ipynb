{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Detection of Indian Actors - version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2.0 adds to the previous version the following features/components/expts. :\n",
    " * Autoencoders\n",
    " * Advanced Activation Layers (LeakyReLU, PReLU, ELU)\n",
    " * Neural network architecture and hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from PIL import Image\n",
    "import cv2 #OpenCV library\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/manikant/Documents/Analytics Vidhya/Age detection/Age-Detection-of-Indian-Actors'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_HOME_DIR = os.getcwd()\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the images and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>377.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17814.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21283.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16496.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4487.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Class\n",
       "0    377.jpg  MIDDLE\n",
       "1  17814.jpg   YOUNG\n",
       "2  21283.jpg  MIDDLE\n",
       "3  16496.jpg   YOUNG\n",
       "4   4487.jpg  MIDDLE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19906</td>\n",
       "      <td>19906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>5351.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID   Class\n",
       "count      19906   19906\n",
       "unique     19906       3\n",
       "top     5351.jpg  MIDDLE\n",
       "freq           1   10804"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIDDLE    10804\n",
       "YOUNG      6706\n",
       "OLD        2396\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting path for images\n",
    "TRAIN_PATH = DATA_HOME_DIR + '/Train/'\n",
    "TEST_PATH = DATA_HOME_DIR + '/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read images as arrays\n",
    "def read_image(img_path, mode = 'color', resize = False, size = 32):\n",
    "    '''\n",
    "    Default mode is : color(BGR) --> color(RGB)\n",
    "    Other modes allowed are : 'grayscale' and 'include_opacity'\n",
    "    '''\n",
    "    if mode == 'grayscale':\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    elif mode == 'include_opacity':\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    else:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if resize == True:\n",
    "        img = cv2.resize(img, (size, size))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19906/19906 [00:22<00:00, 883.16it/s]\n",
      "100%|██████████| 6636/6636 [00:07<00:00, 849.84it/s]\n"
     ]
    }
   ],
   "source": [
    "## Storing all images as list of arrays\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for img in tqdm(train['ID'].values):\n",
    "    train_data.append(read_image(TRAIN_PATH + '{}'.format(img), resize = True, size = 32))\n",
    "    \n",
    "for img in tqdm(test['ID'].values):\n",
    "    test_data.append(read_image(TEST_PATH + '{}'.format(img), resize = True, size = 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking dimensions and aspect ratio of original images (without resizing)\n",
    "rows = [x.shape[0] for x in (train_data+test_data)]\n",
    "cols = [x.shape[1] for x in (train_data+test_data)]\n",
    "channels = [x.shape[2] for x in (train_data+test_data)]\n",
    "aspect_ratio = [x.shape[0]/x.shape[1] for x in (train_data+test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. and Max. rows = 32 and 32 respectively \n",
      "Min. and Max. cols = 32 and 32 respectively \n",
      "Min. and Max. channels = 3 and 3 respectively \n",
      "Min. and Max. aspect ratio = 1.0 and 1.0 respectively \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking range of the dimesnions and aspect ratio\n",
    "print(\"Min. and Max. rows = {} and {} respectively \\nMin. and Max. cols = {} and {} respectively \\nMin. and Max. channels = {} and {} respectively \\nMin. and Max. aspect ratio = {} and {} respectively \\n\"\n",
    "      .format(min(rows), max(rows), min(cols), max(cols), min(channels), max(channels), min(aspect_ratio), max(aspect_ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb17b47cf8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Displaying random images without resizing\n",
    "i = np.random.choice(np.arange(len(train_data)))\n",
    "\n",
    "plt.title('{} - {} ; shape : {}'.format(train['ID'].values[i], y_train[i], train_data[i].shape))\n",
    "plt.imshow(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the images\n",
    "X_train = np.array(train_data, np.float32) / 255.\n",
    "X_test = np.array(test_data, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding image labels into target variables (categorical)\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "y_train = ohe.fit_transform(y_train.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network models - CNNs and its variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU, ELU\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras backend is set to Theano in keras.json file, hence external conversion to Tensorflow is required. In case, the keras backend is TensorFlow, by default, this will not be required.\n",
    "Use keras.backend.backend() or K.backend() to know the current backend.\n",
    "\n",
    "We use Keras with Theano backend and 'tf' image_dim_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-537d3bad36a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KERAS_BACKEND'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "reload(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 : CNN with BatchNorm + with/without Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the same as the ones used in the previous version of this project - Stacked CNN with BatchNormalization layers and Dropout. It can be used with either of the training methods which are shown below in the following sections.\n",
    "\n",
    "A little bit of architecture tuning was tried out by throwing in a couple more Dropout/BatchNorm/Dense/Conv-Pool layers and/or by changing the dropout probability, number of filters, optimizer and other such hyperparameters to arrive at the below model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (32,32,3)),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(384, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 384)               614784    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1155      \n",
      "=================================================================\n",
      "Total params: 682,287\n",
      "Trainable params: 681,897\n",
      "Non-trainable params: 390\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Same as Model 1, but with image size resized to 128 and hence, more Conv-Pool layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few experiments tried were around hyperparameter tuning of these CNN networks.\n",
    "\n",
    "__Hyper-parameters__ : \n",
    "* *Batch Size ($32, 64, 128$ etc.)* : Batch sizes are selected keeping in mind the memory constraints of the hardware being used. Smaller batch sizes take a little longer to train, but are the only solution if our inputs are large. Different batch-sizes performed similarly, but a batch size of 32 turned out to be most suitable keeping all considerations in mind\n",
    "\n",
    "* *Image size ($128X128, 64X64, 32X32$)* : Most input images are large enough (>> 32X32). Hence, using an input image size of 128X128 seems the best option. Resizing the images to 32X32 causes significant blurring in most of the images. This might affect the accuracy of the trained model. However, a network with input images of size 128X128 takes ~25x longer to train than a similar 32X32 network. I tried using 128X128 images, albeit with more Conv-Pool layers, but the resultant accuracy wasn't significantly better. Hence, switched to 32X32. Even networks with 64X64 sized images didn't result in a significant gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (128,128,3)),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 : Stacked CNN + BatchNorm/Dropout with Advanced Activation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a ReLU activation generally works with a stacked CNN model (there's enough literature to prove its superiority over sigmoid and tanh), since the input images were significantly downsampled and hence, highly blurred, I tried using other activation layers as an alternative to ReLU to see if we can arrive at better weights.\n",
    "\n",
    "The other activations tried were :\n",
    "\n",
    "* Leaky ReLU (with different alpha values)\n",
    "* PReLU\n",
    "* ELU\n",
    "\n",
    "Leaky ReLU with a default alpha of 0.3 used in conjunction with Model 1's architecture helped reach ~75% test accuracy after 10-15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (32,32,3)),\n",
    "        Convolution2D(32,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(384, activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_13 (Batc (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 384)               614784    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 1155      \n",
      "=================================================================\n",
      "Total params: 682,287\n",
      "Trainable params: 681,897\n",
      "Non-trainable params: 390\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section contains a function for learning rate decay / annealing along with 3 different ways of training.\n",
    "\n",
    "* Without data augmentation - Doesn't work well mostly. Overfits easily with most architectures\n",
    "* Data augmentation without cross-validation\n",
    "* Data augmentation with cross-validation\n",
    "\n",
    "Data augmentation helps in reducing overfitting and also helps in improving the accuracy for this particular problem, since most of the images are noisy (affected by blur, poor illumination, scaling effects etc.). I separated the cross-validation part so as to quickly try out different models with only one validation set. Once we finalize on a few architectures/set of hyperparameters, learning rate schedule, we can perform cross-validation to get a better estimation of validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a custom function for learning rate decay/annealing\n",
    "def lr_decay(start, stop, div_step_1, div_step_2 = 2) :\n",
    "    k = 1\n",
    "    while start >= stop:\n",
    "        yield start\n",
    "        if k==1 :\n",
    "            start/= div_step_1\n",
    "        else : start/= div_step_2\n",
    "        k = k * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data augmentation is not required, we can simply use the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15924 samples, validate on 3982 samples\n",
      "Epoch 1/2\n",
      "15924/15924 [==============================] - 84s 5ms/step - loss: 1.2725 - acc: 0.5794 - val_loss: 0.8078 - val_acc: 0.6374\n",
      "Epoch 2/2\n",
      "15924/15924 [==============================] - 87s 5ms/step - loss: 0.7821 - acc: 0.6576 - val_loss: 0.6991 - val_acc: 0.6986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3bd9cda0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable /= will be deprecated. Use variable.assign_div if you want assignment to the variable value or 'x = x / y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights as a form of model checkpointing\n",
    "model.save_weights('age-detection_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data augmentation without cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : \n",
    "\n",
    "* When using networks with large images(e.g. 128 X 128), batch_size for train and validation set should be kept low, so as to fit the images in the RAM/vRAM\n",
    "\n",
    "* Each epoch with 128X128 images took ~4800-5200 secs, which makes it infeasible to train for a long time. Either use a faster GPU, or resize all images to 32X32 and train the model on blurred images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_trn, X_valid, y_trn, y_valid = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.25,\n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip = True)\n",
    "batches = gen.flow(X_trn, y_trn, batch_size = 64)\n",
    "val_batches = gen.flow(X_valid, y_valid, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "248/248 [==============================] - 76s 308ms/step - loss: 0.7704 - acc: 0.6627 - val_loss: 0.7235 - val_acc: 0.6802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3c6fc5f8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, (X_trn.shape[0]//batches.batch_size), epochs=1,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "248/248 [==============================] - 87s 349ms/step - loss: 0.7417 - acc: 0.6742 - val_loss: 0.7474 - val_acc: 0.6625\n",
      "Epoch 2/2\n",
      "248/248 [==============================] - 89s 360ms/step - loss: 0.7360 - acc: 0.6732 - val_loss: 0.7219 - val_acc: 0.6881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3c6fc668>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch = (X_trn.shape[0]//batches.batch_size), epochs=2,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]/val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LR decay can also be used for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.005\n",
      "Epoch 1/1\n",
      "248/248 [==============================] - 77s 310ms/step - loss: 0.7190 - acc: 0.6890 - val_loss: 0.7313 - val_acc: 0.6880\n",
      "Learning rate = 0.0025\n",
      "Epoch 1/1\n",
      "248/248 [==============================] - 99s 401ms/step - loss: 0.7141 - acc: 0.6937 - val_loss: 0.6833 - val_acc: 0.7034\n",
      "Learning rate = 0.0005\n",
      "Epoch 1/1\n",
      "248/248 [==============================] - 90s 365ms/step - loss: 0.7162 - acc: 0.6932 - val_loss: 0.6869 - val_acc: 0.7044\n",
      "Learning rate = 0.00025\n",
      "Epoch 1/1\n",
      "248/248 [==============================] - 97s 390ms/step - loss: 0.7057 - acc: 0.6992 - val_loss: 0.7088 - val_acc: 0.7029\n"
     ]
    }
   ],
   "source": [
    "for i in lr_decay(0.005, 0.0002, 2, 5):\n",
    "    model.optimizer.lr = i\n",
    "    print(\"Learning rate = \" + str(i))\n",
    "    model.fit_generator(batches, (X_trn.shape[0]//batches.batch_size), epochs=1,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights as a form of model checkpointing\n",
    "model.save_weights('leakyReLU_age-detection_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data augmentation with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 102)\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=102)\n",
    "fold_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold 1\n",
      "Epoch 1/3\n",
      "497/497 [==============================] - 87s 176ms/step - loss: 0.7653 - acc: 0.6694 - val_loss: 0.7457 - val_acc: 0.6729\n",
      "Epoch 2/3\n",
      "497/497 [==============================] - 84s 170ms/step - loss: 0.7451 - acc: 0.6780 - val_loss: 0.6980 - val_acc: 0.7019\n",
      "Epoch 3/3\n",
      "497/497 [==============================] - 84s 169ms/step - loss: 0.7229 - acc: 0.6916 - val_loss: 0.6946 - val_acc: 0.6941\n",
      "Fitting fold 2\n",
      "Epoch 1/3\n",
      "497/497 [==============================] - 99s 199ms/step - loss: 3.1175 - acc: 0.5272 - val_loss: 0.8606 - val_acc: 0.6139\n",
      "Epoch 2/3\n",
      "497/497 [==============================] - 140s 282ms/step - loss: 0.9190 - acc: 0.5951 - val_loss: 0.7933 - val_acc: 0.6502\n",
      "Epoch 3/3\n",
      "270/497 [===============>..............] - ETA: 58s - loss: 0.8487 - acc: 0.6307"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in skf.split(X_train,np.zeros(shape=(X_train.shape[0], 1))):\n",
    "    print(\"Fitting fold %d\" %fold_num)\n",
    "    \n",
    "    # Data augmentation image generator\n",
    "    gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.25,\n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip = True)\n",
    "    batches = gen.flow(X_train[train_idx], y_train[train_idx], batch_size = 32)\n",
    "    val_batches = gen.flow(X_train[val_idx], y_train[val_idx], batch_size = 64)\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit_generator(batches, steps_per_epoch = (X_trn.shape[0]//batches.batch_size), epochs=3,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))\n",
    "    \n",
    "    # Recompiling the model with initial weights\n",
    "    model.load_weights('initial_weights.h5')\n",
    "    \n",
    "    fold_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders with ConvNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Building the autoencoder pipeline\n",
    "\n",
    "input_img = Input(shape=(32, 32, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (8, 4, 4, 3) i.e. 384-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding = 'same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Composing the autoencoder model and compiling it\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') # can change optimizer to 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 8)           1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 16)          1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 3)         867       \n",
      "=================================================================\n",
      "Total params: 13,939\n",
      "Trainable params: 13,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorboard to visualize the training process\n",
    "# Run this in the terminal\n",
    "!tensorboard --logdir = /tmp/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19906 samples, validate on 6636 samples\n",
      "Epoch 1/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.6284 - val_loss: 0.6159\n",
      "Epoch 2/10\n",
      "19906/19906 [==============================] - 120s - loss: 0.6025 - val_loss: 0.5934\n",
      "Epoch 3/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.5946 - val_loss: 0.5942\n",
      "Epoch 4/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.5913 - val_loss: 0.5889\n",
      "Epoch 5/10\n",
      "19906/19906 [==============================] - 124s - loss: 0.5891 - val_loss: 0.5887\n",
      "Epoch 6/10\n",
      "19906/19906 [==============================] - 123s - loss: 0.5873 - val_loss: 0.5862\n",
      "Epoch 7/10\n",
      "19906/19906 [==============================] - 125s - loss: 0.5860 - val_loss: 0.5875\n",
      "Epoch 8/10\n",
      "19906/19906 [==============================] - 125s - loss: 0.5852 - val_loss: 0.5847\n",
      "Epoch 9/10\n",
      "19906/19906 [==============================] - 124s - loss: 0.5844 - val_loss: 0.5851\n",
      "Epoch 10/10\n",
      "19906/19906 [==============================] - 123s - loss: 0.5839 - val_loss: 0.5834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1071911a90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the autoencoder model\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs= 10,\n",
    "                batch_size= 64,\n",
    "                shuffle= True,\n",
    "                validation_data= (X_test, X_test))\n",
    "                #callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19906 samples, validate on 6636 samples\n",
      "Epoch 1/3\n",
      "19906/19906 [==============================] - 109s - loss: 0.5836 - val_loss: 0.5829\n",
      "Epoch 2/3\n",
      "19906/19906 [==============================] - 109s - loss: 0.5834 - val_loss: 0.5834\n",
      "Epoch 3/3\n",
      "19906/19906 [==============================] - 116s - loss: 0.5833 - val_loss: 0.5835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10c4061b10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs= 3,\n",
    "                batch_size= 128,\n",
    "                shuffle= True,\n",
    "                validation_data= (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6636, 32, 32, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting predicted category numbers to predicted labels\n",
    "unique_labels = np.unique(train['Class'].tolist())\n",
    "pred_labels = unique_labels[predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f317283c590>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN/9rkc7OudYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f317283c6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's see what our classifier predicts on test images\n",
    "# Random predictions\n",
    "i = np.random.choice(np.arange(len(test_data)))\n",
    "plt.title('{} - Predicted class : {}'.format(test['ID'].values[i], pred_labels[i]))\n",
    "plt.imshow(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare submission file\n",
    "subm = pd.DataFrame({'Class':pred_labels, 'ID':test.ID})\n",
    "subm.to_csv('sub03_2_leakyReLU.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
